# Chapter 14. Managing pods' computational resources

### 주요 내용

- ...

각 pod 가 어느 정도의 자원(CPU/메모리)을 소모할지 파악하고, 이를 적절히 제한하는 것은 pod 정의에서 굉장히 중요한 부분이다. 

## 14.1 Requesting resources for a pod's containers

Pod 를 생성할 때 **requests** 와 **limits** 를 정할 수 있다.

- requests: 컨테이너가 필요로 하는 CPU와 메모리 양을 설정
- limits: 컨테이너가 최대로 사용할 수 있는 CPU와 메모리를 설정

이는 각 컨테이너 별로 설정할 수 있으며, pod 의 requests/limits 는 각 컨테이너들의 requests/limits 의 합이 된다.

### 14.1.1 Creating pods with resource requests

리소스 requests 를 가진 pod 를 생성하는 것은 무척 간단하다.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: requests-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:
      requests:
        cpu: 200m           # 200 millicore
        memory: 10Mi        # 메모리 10 MiB
```

> 200 millicore 라는 것은 한 CPU 코어의 1/5를 사용한다는 것이고 이는 곧 CPU 타임의 1/5 만 사용한다는 의미이다.

만약 requests 를 정하지 않으면 이 컨테이너가 자원을 임의로 할당받아도 무관하다는 뜻이 되기 때문에, 최악의 경우에는 자원을 사용하지 못하게 될 수도 있음에 유의해야 한다.

Pod 를 실행하고 `top` 을 쳐보면 자원을 얼마나 소비하고 있는지 확인할 수 있다.

```
Mem: 10819360K used, 5510064K free, 211100K shrd, 173304K buff, 5530868K cached
CPU:  4.0% usr  9.6% sys  0.0% nic 86.1% idle  0.1% io  0.0% irq  0.0% sirq
Load average: 1.21 1.08 0.79 3/2346 46
  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND
    1     0 root     R     1316  0.0   5 12.4 dd if /dev/zero of /dev/null
   41     0 root     R     1324  0.0   1  0.0 top
```

막상 확인해 보면 CPU는 12.5%를 사용하고 있는데, 이는 현재 실행 환경의 CPU 코어 수가 8개여서 그런 것이고, `top` 을 실행한 뒤 키보드 `1` 을 눌러보면 각 코어별 사용률을 볼 수 있다.

```
CPU5: 33.3% usr 66.6% sys  0.0% nic  0.0% idle  0.0% io  0.0% irq  0.0% sirq
```

한편 pod 정의에서는 200 millicore 로 설정했었는데, 정작 확인해보니 한 코어를 full 로 사용하고 있는 것을 확인할 수 있었다. 

이처럼 requests 만 정의하는 경우에는 제한이 없으므로 컨테이너는 원하는 만큼 자원을 사용할 수 있게 된다.

### 14.1.2 Understanding how resource requests affect scheduling

즉, requests 를 설정하게 되면 **pod 가 요구하는 최소 자원**을 설정하는 셈이 된다. 그렇기 때문에 이는 pod scheduling 에도 영향을 주게 된다. 최소 자원을 만족시키지 못하는 노드에 pod 를 schedule 해서는 안 되기 때문이다.

Scheduling 에서 유의할 점은 Scheduler 가 pod 를 띄울 노드를 선택할 때, scheduling 당시에 소비되고 있는 자원들의 상태를 확인하는 것이 아니라, **노드에 존재하는 pod 들의 자원 requests 총합**을 확인한다는 점이다.

> 이렇게 해야 각 pod 의 requests 를 모두 만족시킬 수 있기 때문이다. Scheduling 당시에 자원이 적게 소모된다고 해서 괜찮다고 생각하고 scheduling 해버리면 나중에 자원이 부족해진다. 한편 자원을 request 한 값보다 많이 사용하고 있는 경우에는, CPU 는 살짝 throttling 을 걸어서 사용률을 낮추면 그만이다. (request 이하로 떨어지지는 않을 것) 반면 메모리의 경우는 너무 많이 쓰고 있으면 컨테이너가 잘 free 해줘야 한다. Free 되지 않으면 pod 가 scheduling 은 됐지만... 문제가 생긴다.

Scheduler 가 pod 의 requests 를 만족하는 노드를 찾았다면, 이제 실제로 할당을 하면 되는데, 이 때 조건에 맞는 노드가 여러 개 일수도 있다. 이 경우 우선순위를 두는 방법이 2가지 있다.

- `LeastRequestedPriority`: 여유 자원이 많은 노드를 선호하는 방법
- `MostRequestedPriority`: 여유 자원이 적은 노드를 선호하는 방법

> `MostRequestedPriority` 의 경우 클라우드에서 사용하는 노드 수를 줄이기 위해 사용할 수 있다. 물론 여러 노드를 띄워서 각 노드가 여유롭게 작업하도록 하면 좋겠지만, 비용을 절감하기 위해 노드의 자원을 최대한 전부 사용하도록 선택하는 경우도 있다.

#### 노드의 자원 양 확인하는 방법

`kubectl describe nodes` 를 하면 중간에 `Capacity`, `Allocatable` 부분이 있다.

```
Capacity:
  cpu:                8
  ephemeral-storage:  102686648Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16329424Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  102686648Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16329424Ki
  pods:               110
```

참고로 `Non-terminated Pods` 부분을 보면 `CPU Requests` column 이 있는데, 직접 생성한 pod 이외에도 `kube-system` 과 관련된 pod 들이 실행 중이며, 이 pod 들도 requests 가 있다는 점을 기억해 두어야 한다. 시스템 pod 들이 일부 리소스를 사용 중이므로 사용자가 생성한 컨테이너 만으로 CPU 전체를 쓸 수는 없다.

### 14.1.3 Understanding how CPU requests affect CPU time sharing

앞에서 requests 만 설정하고 limits 를 두지 않았기 때문에 CPU 사용에 제한이 없다고 했는데, 만약 여러 개 pod 를 띄우게 되면 여분의 CPU time 은 각 pod 의 request 양의 비율에 맞게 배분된다.

예를 들어 2개의 pod 이 각각 200m, 1000m 의 CPU 를 요청했다면, 남는 CPU 는 1:5 로 비례배분되어 각 pod 이 사용하게 된다.

하지만 항상 이렇게 되는 것은 아니고, 한 pod 가 idle 상태일 때 다른 pod 에서 CPU 를 더 많이 가져가서 쓰려고 하면 사용할 수 있으며, idle 상태에서 벗어나 CPU 를 요청하게 되면 더 많이 쓰던 pod 에는 throttling 이 걸리게 된다.

### 14.1.4 Defining and requesting custom resources

Kubernetes 에서는 사용자 지정 resource 를 정의해서 requests 에 포함할 수 있다. (Extended Resources since version 1.8)

> 어떻게 쓰는지 잘 모르겠다!

## Discussion & Additional Topics


